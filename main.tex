%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------
\documentclass[aspectratio=169]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
\setbeamertemplate{footline}[frame number] % To replace the footer line in all slides with a simple slide count uncomment this line


\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% \usepackage[style=authoryear]{biblatex}
\usepackage{biblatex}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%\usepackage {tikz}
\usepackage{tkz-graph}
\GraphInit[vstyle = Shade]
\tikzset{
  LabelStyle/.style = { rectangle, rounded corners, draw,
                        minimum width = 2em, fill = yellow!50,
                        text = red, font = \bfseries },
  VertexStyle/.append style = { inner sep=5pt,
                                font = \normalsize\bfseries},
  EdgeStyle/.append style = {->, bend left} }
\usetikzlibrary {positioning}
%\usepackage {xcolor}
\definecolor {processblue}{cmyk}{0.96,0,0,0}


% My macros
\usepackage{lipsum}
\usepackage{caption}
\usepackage{pgfpages}
\setbeameroption{show notes}
\setbeameroption{show notes on second screen=right}

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}


\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\def\trainingset{\{x_i,y_i\}_{i=1}^{N}}
\def\pNMLSingle{p_{\hat{\theta}(z^N,x,y)} (y|x)}
\newcommand{\SubItem}[1]{
    {\setlength\itemindent{15pt} \item[-] #1}
}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand\tableref{Table~\ref}
\def\Dc{\mathcal{D}_c}
\def\Du{\mathcal{D}_u}
\newcommand\cC{{\cal C}}
\newcommand{\minisection}[1]{\vspace{2mm}\noindent{\textbf{#1.}}}
\newcommand\koby[1]{\textcolor{blue}{[KB: #1]}}
\newcommand\meir[1]{\textcolor{red}{[MF: #1]}}
\newcommand{\ignore}[1]{}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Short title]{Universal Learning of Individual Data} % The short title appears at the bottom of every slide, the full title is only on the title page


\date{September 2020} % Date, can be changed to a custom date

\subtitle{Ph.D. Research Proposal}
\author[]{\fontsize{12}{14}\selectfont Koby Bibas 
\texorpdfstring{\\ Advisor: Prof.~Meir~Feder}{}}


\institute[\large Tel Aviv University]
{\includegraphics[scale=0.075]{TAU-NEW-LOGO.jpg}} 

\bibliography{references.bib}
\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

% \begin{frame}
% \frametitle{Overview} % Table of contents slide, comment this block out to remove it
% \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
% \end{frame}

%--------------------------------------------------
%	PRESENTATION SLIDES
%--------------------------------------------------
\begin{frame}{Outline}
\tableofcontents
\note{
We begin with describing the setting: the supervised learning and what is individual data. \\
Then we show the pNML which is the min-max solution of this setting \\
Next, we present initial results that are the analytical solution of the pNML in the linear regression case and efficient algorithm for Deep Neural Networks. \\
Finally we discuss future research where we focus on over-parameterized models. }
\end{frame}



% ------------------------------ %
\section{Introduction}
% ------------------------------ %
\begin{frame}[c]
\begin{center}
\Huge Introduction
\end{center}
\end{frame}



\subsection{The supervised learning setting}
\begin{frame}{The supervised learning setting}
\begin{itemize}
\setlength\itemsep{1.5em}
\item A training set $z^N = \{(x_i,y_i)\}_{i=1}^N$ is given. 
\item The goal is to predict the label $y$ of test data $x$. 
\item We assume to have a model class $\Theta$.
\item A predictor $q_{\theta}(\cdot|x), \ \theta \in \Theta$, is evaluates using the log-loss  
    \begin{equation}
    \mathcal{L}(q_\theta, x, y) = -\log {q_\theta(y|x}).
    \end{equation}
\item The Empirical Maximum Likelihood (ERM) learner
\begin{equation}
    \theta_{ERM} = \argmin_{\theta \in \Theta} \sum_{i=1}^N \mathcal{L}(q_\theta, x_i, y_i)
\end{equation}
\end{itemize}
\note{The ERM is the learner that has the minimal loss on the training set}
\end{frame}



\begin{frame}{Learning of individual data}
\begin{itemize}
\setlength\itemsep{0.5em}
\item Individual settings $x,y \not\sim p(x,y)$.  
\item Competing against a Genie who knows the true label
\SubItem {Constrained to a certain family $\{p_{\theta}(\cdot|x), \ \theta \in \Theta\}$}.
\SubItem {Does not know which sample is the test sample}
\item The genie parameters:
    \begin{equation}
        \hat{\theta}(z^N,x,y) = \argmin_{\theta \in \Theta}\left[\mathcal{L}(q_\theta, x, y) +  \sum_{i=1}^N \mathcal{L}(q_\theta, x_i, y_i) \right]
    \end{equation}
\item The regret
    \begin{equation}
        R(z^N,x,y,q) = \log \frac{p_{\hat{\theta}(z^N,x,y)}(y|x)}{q_(y|x;z^N)}.
    \end{equation}
\end{itemize}
\note{
There is no stochastic relationship between x and y. \\
The genie is too powerful, so we need to constraint it.\\
We reduce the genie capacity. \\
The regret is the logloss difference between the given learner and the genie. \\
Our goal is to find a learner that minimizes the regret.}
\end{frame}



\subsection{The predictive Normalized Maximum Likelihood}
\begin{frame}{The Predictive Normalized Maximum Likelihood (pNML)}
The pNML minimizes the regret for any outcome $y$ 
\begin{equation} 
    \Gamma = \min_q \max_y R(z^N,x,y,q) = \min_q \max_y \log \frac{p_{\hat{\theta}(z^N,x,y)}(y|x)}{q_(y|x;z^N)}.
\end{equation}
The learner
\begin{equation}
    q_{\mbox{\tiny{pNML}}}(y|x;z^N)=\frac{\pNMLSingle}{\sum_{y\in {\cal Y}} \pNMLSingle}.
\end{equation}
The regret
\begin{equation}
    \Gamma = \log K = \log \left\{ \sum_{y\in {\cal Y}} \pNMLSingle \right\}.
\end{equation}
\blfootnote{Y.  Fogel  and  M.  Feder, ``Universal Supervised Learning for Individual Data", ISIT~2019}
\note{
We call the denominator of eq. 6 ``the normalization factor''. \\
The log normalization factor is the regret. \\
The numerator for the true label is the genie.
}
\end{frame}




\begin{frame}{The pNML procedure}
Inputs: A training set $z^N=\{(x_i,y_i)\}_{i=1}^N$, test data $x$ and hypothesis class $\Theta$. 
\begin{enumerate}
    \item Assume the label of the test data is $\Tilde{y}$, add it to the training 
    \begin{equation*}
        z^{N+1} = z^N \cup (x,\Tilde{y})
    \end{equation*}
    \item Fit a learner
    \begin{equation*}
        \hat{\theta}(z^N,x,y) = \argmin_{\theta \in \Theta} \bigg[-\sum_{(x_i,y_i)\in z^{N+1}} \log p_{\theta}(y_i|x_i) \bigg]
    \end{equation*}
    \item Predict the assumed label 
    \begin{equation*}
    p(\Tilde{y}) = p_{\hat{\theta}}(\Tilde{y}|x)            
    \end{equation*}
    \pause
    \item Repeat the process for all possible labels.
\end{enumerate}
\end{frame}



\begin{frame}{The pNML procedure}
\begin{enumerate}
\setlength\itemsep{2em}
\setcounter{enumi}{4}
\item Compute normalization factor
    \begin{equation*}
        K = \sum_{\Tilde{y}}p(\Tilde{y})
    \end{equation*}
\item Return the regret
\begin{equation*}
    \Gamma = \log K.  
\end{equation*}
\item Normalize the probability assignment and return the pNML learner
    \begin{equation*}
        q_{\mbox{\tiny{pNML}}}(y) = \frac{1}{K}p(y).
    \end{equation*}
\end{enumerate}
\note{Let's do 2 examples}
\end{frame}

% \begin{frame}{Dog vs cat example}
% \centering
% \includegraphics[width=0.45\textwidth]{figures/dog_vs_cat_example_input.jpg}
% \pause
% \includegraphics[width=0.85\textwidth]{figures/dog_vs_cat_example_alg.jpg}
% \end{frame}


\begin{frame}{A binary sequence example}
\centering
\begin{Example}
    Given a binary sequence  
    \begin{equation}
    n_0 = \#zeros \quad n_1 = \#ones \quad n = n_0 + n_1.
    \end{equation}
    The goal is to predict the next digit. \\
    \begin{equation*}
    1110010111 \ ?   
    \end{equation*}
    \pause
    \centering
    \begin{tabular}{l|c|c|c}
        & p(y=0)    & p(y=1) & sum    \\
        \hline
        ERM    &   $n_0/n$        &  $n_1/n$ & 1   \\
        \hline
        pre-norm pNML &   $(n_0+1)/(n+1)$        &  $(n_1+1)/(n+1)$ & $(n+2)/(n+1)$  \\
        \hline
        pNML &   $(n_0+1)/(n+2)$        &  $(n_1+1)/(n+2)$ & 1  \\
    \end{tabular}
\end{Example}
\end{frame}

% ------------------------------ %
\section{Our preliminary results}
% ------------------------------ %
\begin{frame}[c]
\begin{center}
\Huge Our preliminary results
\end{center}
\end{frame}



% ---------------------------- %
\subsection{Linear regression}
% ---------------------------- %
\begin{frame}{Linear regression setting}
    Given N training samples $z^N=\{(x_i,y_i)\}_{i=1}^N$, the model:
    \begin{equation}
        y_i =x_i^\top \theta + e_i, \;\; \ e_i \sim \mathcal{N}(0,\sigma^2). 
    \vspace{5mm}
    \end{equation}
    The data matrix and the label vector:
    \begin{equation*}
        X_N = \begin{bmatrix} x_1 & \dots & x_N  \end{bmatrix}^\top \in \mathcal{R}^{N \times M} 
        \;\;\ \ 
        Y_N = \begin{bmatrix} y_1 & \dots & y_N \end{bmatrix}^\top \in \mathcal{R}^{N}
    \vspace{5mm}
    \end{equation*}
    The least square solution:
    \begin{equation}
        \theta_N = \arg\min_{\theta\in\Theta} \sum_{i=1}^N \left(y_i - x_i^\top \theta \right)^2 = (X_N^\top X_N)^{-1}X_N^\top Y_N.
    \end{equation}
    The ERM prediction:
    \begin{equation*}
        \hat{y} = x^\top \theta_{N}.
    \end{equation*}{}
    
\note{We assume a linear model. \\
The ERM is the learner that as the minimum loss over the training set. \\
In least squares, minimize the logloss equals minimum MSE. 
}
\end{frame}

\begin{frame}{The pNML learner for linear regression}
    \begin{theorem}
    The pNML solution for linear regression is
    \begin{equation}
    q_{\mbox{\tiny{pNML}}}(y | x; z^N) = \frac{1}{\sqrt[]{2\pi\sigma^2 K^2}}\exp\left\{-\frac{1}{2\sigma^2 K^2}\left(y - x^\top \theta_N \right)^2\right\}.
    \end{equation}
    where $K$ is the normalization factor, and 
    the associated regret is:
    \begin{equation}
    \Gamma = \log K = \log \frac{1}{1 - x^\top (X_{N+1}^\top X_{N+1} )^{-1} x }, \;\;\;X_{N+1} = \begin{bmatrix} X_N \\ x^\top  \end{bmatrix}.  
    \end{equation}
    \end{theorem} 
    \blfootnote{Bibas, K., Fogel, Y., Feder, M. (2019, July). ``A New Look at an Old Problem: A Universal Learning Approach to Linear Regression'', ISIT 2019}
    
\note{
The pNML probability assignment is a normal distribution around the ERM prediction. \\
The variance is scaled by the normalization factor. \\
$X_{N+1}$ is the data matrix after adding the test data to the training.
}
\end{frame}

\begin{frame}{Sketch of the proof}
\begin{itemize}
\item Assuming that the test label is known
    \begin{equation*}
    X_{N+1} = \begin{bmatrix} x_1 & \dots & x_N & x \end{bmatrix}^\top, \;\;\ \ 
    Y_{N+1} = \begin{bmatrix} y_1 & \dots & y_N & y \end{bmatrix}^\top 
    \end{equation*}
\item Fit a least squares learner: 
    \begin{equation*}
    \theta_{N+1} = (X_{N+1}^\top X_{N+1})^{-1} X_{N+1}^\top Y_{N+1}
    \end{equation*}
    The genie prediction of the test label
    \begin{equation} \label{genie}
    p_{\theta_{N+1}}(y) =
    \frac{1}{\sqrt[]{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}\big(y- x^\top \theta_{N+1} \big)^2\right\}.
    \end{equation}
\item Using the recursive least square formulation
    \begin{equation}
    \theta_{N+1} = \theta_{N} + \left(X_{N+1}^\top X_{N+1}\right)^{-1} x (y -  x^\top \theta_N)
    \end{equation}
\item Substituting in (\ref{genie}) and normalizing proves the Theorem.
\end{itemize}
\end{frame}


\begin{frame}{The leverage}
\begin{itemize}
\setlength\itemsep{1.0em}
\item The normalization factor $K$ is attained by integrating over all $y$'s
    \begin{equation}
        K 
        = \int_{-\infty}^{\infty} p_{\theta_{N+1}}(y) dy 
        = \frac{1}{1 - x^\top \left( X_{N+1}^\top X_{N+1} \right)^{-1} x }
    \end{equation}
\item The following quantity is named ``Leverage''
    \begin{equation*}
        h_{ii}=\left [ X_{N+1} (X_{N+1}^\top  X_{N+1})^{-1}X_{N+1}^\top \right]_{ii}.
    \end{equation*}
    $0\leq h_{ii} \leq 1$, large leverage value implies an observation outlier.
    \item Thus: \begin{equation} K= \frac{1}{1-h_{N+1,N+1}} \end{equation}
\end{itemize}
\note{
Let's look at the normalization factor. \\
The leverage indicates the distance of the data from the mean. \\
If the test sample is far from the mean, it has a value of 1 and infinite regret. \\
If the test sample is exactly in the mean, it has a value of 1 and we get regret of 0. }
\end{frame}

\begin{frame}{The learnable space}
\begin{theorem}
    Let $R_N=\frac{1}{N} X_N^\top X_N  $ be the empirical correlation matrix of the training set, with eigenvectors/eigenvalues decomposition: 
    \begin{equation*}
    R_N = \begin{bmatrix} u_1 & \hdots & u_M \end{bmatrix}
    \begin{bmatrix}
    h_1^2 & \hdots & 0 \\
    \vdots & \vdots &  \vdots \\
    0 & \hdots &  h_M^2 \\
    \end{bmatrix}
    \begin{bmatrix}
    u_1^\top \\ \vdots \\ u_M^\top 
    \end{bmatrix}.
    \end{equation*}
    \newline
    The pNML regret is
    \begin{equation}
    \Gamma = \log K = \log \left(1 + \frac{1}{N} \sum_{i=1}^{M} \frac{\left(x^\top u_i\right)^2 }{h_i^2}\right).
    \end{equation}
\end{theorem} 
\pause
{\bf If the test sample $x$ lies in the subspace spanned by the eigenvectors with large eigenvalues, then the model generalizes well.}

\note{When the training set size increases, the regret decreases. }
\end{frame}



\begin{frame}{Linear regression with regularization}
The least squares with $\lambda$ regularization term  solution 
    \begin{align}
    \theta_N = \arg\min_{\theta\in\Theta} \left[ \sum_{i=1}^N \left(y_i - x_i^\top \theta \right)^2 + \lambda ||\theta||_2^2 \right] = (X_N^\top X_N + \lambda I)^{-1} X_N^\top Y_N
    \end{align}
\pause
\begin{theorem}
Denote  $K = \left[1 - x^\top (X_{N+1}^\top X_{N+1} + \lambda I)^{-1} x \right]^{-1}$, the pNML solution is
\begin{equation}
    q_{\mbox{\tiny{pNML}}}(y | x; z^N) = \frac{1}{\sqrt[]{2\pi\sigma^2 K^2}}\exp\left\{-\frac{1}{2\sigma^2 K^2}\left(y-x^\top \theta_N \right)^2\right\}.
\end{equation}
The pNML regret is
\begin{equation}
    \Gamma = \log K = \log \left(1 + \frac{1}{N} \sum_{i=1}^{M} \frac{\left(x^\top u_i\right)^2 }{h_i^2 + \lambda}\right).
\end{equation}
\end{theorem}

\note{When increasing $\lambda$ we get lower regret, however, also bias in the prediction, higher training loss. \\
* Actually, we are closer to the genie but the genie performs worse}
\end{frame}



\begin{frame}{Simulation}
Polynomial fitting
\begin{equation*}
y = \theta_0 + \theta_1 t +  \theta_2 t^2 + \dots +  \theta_9 t^9 + \theta_{10} t^{10} + e
\vspace{10mm}
\end{equation*}
Given a trainset $z^N = \{\left(t_i,y_i\right)\}_{i=1}^{10}$:
\begin{equation*}
X_{N} = \begin{bmatrix} 
1       & \dots & 1            \\
t_1     & \dots & t_{10}          \\
t_1^2   & \dots & t_{10}^2      \\
\vdots  &       & \vdots   \\
t_1^{10}   & \dots & t_{10}^{10} \\ 
\end{bmatrix}^\top, 
\quad \quad
Y_N = \begin{bmatrix} y_1 \\ \vdots \\ y_{10} \end{bmatrix},
\quad \quad
\theta = \begin{bmatrix} \theta_0 \\ \vdots \\ \theta_{10} \end{bmatrix}
\end{equation*}

\note{The simplest linear regression problem is polynomial fitting. \\
We simulate 10 training points. }
\end{frame}



\begin{frame}{The regret simulation}
\begin{figure}
    \vspace{-0.1cm}
    \centering
    \includegraphics<1>[width=0.6\linewidth]{figures/linear_regression_figures/figure_trainset.pdf}
    \pause
    \includegraphics<2->[width=0.6\linewidth]{figures/linear_regression_figures/figure_poly_experiment_prediction_10_deg.pdf}
    \vspace{-0.4cm}
\end{figure}
\pause
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/linear_regression_figures/figure_poly_experiment_regret_10_deg.pdf}
\end{figure}

\note{Each training sample is plotted on the graph. \\
We fit a polynomial function to this training set. \\
We treat each point in the -1 and 1 axis as a test point. \\
We calculate the corresponds regret. \\
For areas around the training sample the regret is low. \\
When moving away from the training samples, the regret increases
}
\end{frame}



\begin{frame}{Simulation with various polynomial degrees}
\begin{figure}
    \vspace{-0.1cm}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/linear_regression_figures/figure_poly_experiment_prediction_all.pdf}
    \vspace{-0.4cm}
\end{figure}
\pause
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/linear_regression_figures/figure_poly_experiment_regret_all.pdf}
\end{figure}

\note{We fit polynomial functions with different degrees. \\
For each polynomial degree and for each point between -1 and 1 we calculate the regret. \\
We get similar behaviour: around the training set the regret is low. \\
When the polynomial degree increases we get higher regret. \\
It may relate to the MDL or oakum razor principle. \\
Any quotations about linear regression?
}
\end{frame}



% \begin{frame}{Summary}
% \begin{itemize}
%     \item The Universal Learning Setting and pNML Recap
%     \SubItem{Supervised learning in the Individual settings}
%     \SubItem{The pNML attains the minmax regret with respect to the Genie}
%   \vspace{5mm}
%     \pause    
%     \item Analytic solution of the pNML for linear regression was shown
%     \item  The pNML regret
%         \begin{equation*}
%     \Gamma = \log K = \log \left(1 + \frac{1}{N} \sum_{i=0}^{M} \frac{\left(x^\top u_i\right)^2 }{\eta_i}\right)
%         \end{equation*}
%     \item {\bf The ``learnability space'':} Generalization can be attained if the test data mostly lies in a small subspace spanned by eigenvectors of the correlation matrix with large eigenvalues
%     \pause
%     \item Simulations of polynomial fitting 
%  \end{itemize}
% \end{frame}

% ------------------------------ %
\subsection{Deep neural networks}
% ------------------------------ %
\begin{frame}{Deep neural networks}
\begin{itemize}
\setlength\itemsep{2em}
\item DNN is a very rich of the hypothesis class. 
\item State of the art DNNs easily fit a random labeling of the training data~\footfullcite{DBLP:conf/iclr/ZhangBHRV17}
\item We obtain the maximum regret when executing the pNML procedure using DNN
    \begin{equation}
    \Gamma = \log \# \textit{class}
    \end{equation}
\item We propose a method that reduced the richness of the DNN hypothesis class
\end{itemize}
\end{frame}

\begin{frame}{Efficient pNML procedure}
\begin{enumerate}
\item Train a DNN model using the standard ERM training procedure.
\item  Extract the features of the trainset $ \{(\phi(x_i),y_i)\}_{i=1}^N$
\end{enumerate}
\vspace{1cm}
\centering
\includegraphics[width=0.8\textwidth]{figures/dnn_feature_extraction.pdf}
\end{frame}

\begin{frame}{Efficient pNML procedure}
\begin{enumerate}
\setcounter{enumi}{3}
\item Add the test instance  $(\phi(x),y)$ to the trainset features with arbitrary choice of $y$
\item Train the single fully connected layer using the ERM procedure
    \begin{equation}
    w_{y} = \argmin_{w} \left[\sum_{i=1}^N  - \log p_{w}\left(y_i|\phi(x_i)\right) 
    - \log p_{w}\left(y|\phi(x)\right) \right].
    \end{equation}
\item Given the trained layer, predict the class it was trained with
\begin{equation}
    p_{y} = p_{w_{y}}(y|\phi(x)).
\end{equation}
\item Repeat 3-5 for every possible label
\item The associated regret is then 
    \begin{equation}
    \Gamma = \log \sum_{y \in \mathcal{Y}} p_{y} 
    = \log \sum_{y \in \mathcal{Y}}  p_{w_{y}}(y|\phi(x)). 
    \end{equation}
\end{enumerate}
\end{frame}

\begin{frame}{Efficient pNML remarks}
\vspace{-2.0cm}
\begin{itemize}
\setlength\itemsep{2em}
\item We define the hypothesis class as one fully connected layer
\item By using one fully connected layer we are in the under-parameterized regime
\item Accelerating the pNML procedure
\end{itemize}
\end{frame}

\begin{frame}{The open-set task}
The goal is to simultaneity:
\begin{itemize}
\item classify correctly image with known class
\item alert when image with unknown is presented
\end{itemize}
\centering
\includegraphics[width=0.7\textwidth]{figures/openset_task.jpg}
\end{frame}


\begin{frame}{Evaluation methodology}
\begin{itemize}
\item  Open-Set Classification Rate (OSCR) evaluation metric
\item  $\mathcal{D}_c$ is a dataset that contains known classes
\item  $S(x)$ is a confidence score for sample x. $T$ is a score threshold.
\item $c$ is the correct class
\item  The Correct Classification Rate (CCR) 
    \begin{equation}  \label{eq:ccr}
    \mathrm{CCR}(T) = \frac{1}{|\Dc|}\bigl|\{x \mid  x \in \Dc \  \wedge 
    \argmax_{y \in \mathcal{Y}} p(y|x) = c \ \wedge 
    S(x) > T \}\bigr|.
    \end{equation}
\item $\mathcal{D}_u$ contains data with unknowns.
\item The False Positive Rate (FPR)
    \begin{equation}  \label{eq:fpr}
        \mathrm{FPR}(T) =  \frac{1}{|\Du|} \left|\{x \mid x \in \Du \wedge S(x) \geq T \}\right|.
    \end{equation}
\end{itemize}
\blfootnote{Akshay Raj Dhamija et al. ``Reducing Network Agnostophobia'' NIPS 2018}

\note{
The CCR is the fraction of sampled from $\Dc$ that we are confident that they are with known class and also classified correctly. \\
The FPR is the fraction of samples from $\Du$ that we thought they had known class.  \\
If we choose the minimal $T$, we get the maximal FPR and CCR that equals to the accuracy of the model on $\Dc$.
}
\end{frame}


\begin{frame}{DNN open-set results}
CCR vs FPR curves of the compared methods with CIFAR10 and CIFAR100  as datasets with known classes. \\
Images from LSUN simulate unexpected inputs. 

\begin{figure}[h]
\includegraphics[width=0.45\textwidth]{figures/dnn_openset_figures/figure_pnml_densenet100_cifar10_lsun.pdf}
\includegraphics[width=0.45\textwidth]{figures/dnn_openset_figures/figure_pnml_wrn16_cifar100_lsun.pdf}
\end{figure}
More results in \fullcite{pNML_neural_networks}

\note{
Left is CIFAR10 dataset as $\Dc$ (dataset with known classes and LSUN with unknowns. The model is DensNet100 \\
Right is CIFAR100 dataset as $\Dc$ (dataset with known classes and LSUN with unknowns. The model is WideResNet16. \\
For maximal FPR we get the accuracy on $\Dc$. \\
We are better than the competitor for all FPR values.
}
\end{frame}

\begin{frame}{Hypothesis class experiment}
We consider the following hypothesis class
\vspace{0.25cm}
\begin{enumerate}
\setlength\itemsep{1em}
\item \emph{0 Layers}: Nothing is trained during the fine-tuning phase. This is the ERM learner.
\item \emph{2 Layers}: The fine-tuning alters only the last two layer.
\item \emph{7 Layers}: When the fine-tuning phase affects all layers.
\end{enumerate}
\vspace{0.5cm}
\centering
\input{tables/num_layers}

\note{
In the efficient pNML scheme we chose a single 1 fully connected later as our hypothesis class. \\
We would like to see how this choice affect the regret and the accuracy. \\
The model is Resnet18 with CIFAR10. \\
You can see that we get the best accuracy for the 2 layers model. \\
As increasing the model capacity, the regret increases as well. 
}
\end{frame}

\begin{frame}{Hypothesis class experiment result}
\includegraphics[width=0.49\textwidth]{figures/dnn_openset_figures/figure_resnet18_regret_histogram_num_layers.pdf}
\includegraphics[width=0.49\textwidth]{figures/dnn_openset_figures/figure_resnet18_loss_histogram_num_layers.pdf}

\note{
The 7 layers model has the highest regret. \\
The 0 layers has the worst case logloss.\\
There is a trade off between the accuracy and the worst-case. The pNML is like an insurance that you pay to minimize the worst case. \\
Questions about DNN?
}
\end{frame}



% ------------------------------ %
\section{Future Research}
% ------------------------------ %
\begin{frame}[c]
\centering
\Huge Future Research Directions
\note{
I'll now discuss future research direction. \\
We especially want to focus on over-parameterized models. Where the number of parameters is greater than the training set size.}
\end{frame}

\subsection{Over-parameterized linear regression}
\begin{frame}{Over-parameterized linear regression}
\begin{itemize}
\setlength\itemsep{1.5em}
\item There is a trade-off between the training set error and the complexity of the model
\item We consider when a perfect fit to training data is compatible with an accurate prediction
\item The minimum norm solution: The solution that attains a perfect fit on the training set and has the minimum norm than all other solutions that have the perfect fit.
\end{itemize}

\note{
The minimum norm solution is the simplest model the attains perfect fit on the training set}
\end{frame}



\begin{frame}{Minimum norm solution in linear regression}
Given a training set $z^N =\{(x_i,y_i)\}_{i=1}^N$:
\begin{equation} \label{eq:trainset_matrix}
X_N = 
\begin{bmatrix}
x_1 & x_2 & \dots & x_N
\end{bmatrix}^\top
\in \mathcal{R}^{N \times M},
\quad
Y_N = \begin{bmatrix} y_1 & y_2 & \dots & y_N \end{bmatrix}^\top
\in \mathcal{R}^{N}.
\end{equation}
The pseudo-inverse~\footnote{Moore–Penrose inverse} of $X_N$ is
\begin{equation} \label{eq:pseudo-inverse}
X_N^+ = 
\begin{cases} 
    (X_N^\top X_N)^{-1} X_N^\top & \textit{Rank}(X_N^\top X_N) = M\\ 
    X_N^\top (X_N X_N^\top )^{-1} & \textit{otherwise} \\   
\end{cases}
\end{equation}
where $X_N^\top X_N \in R^{M \times M}$ and $X_N X_N^\top \in R^{N \times N}$. 
\\
The minimum norm least squares solution takes the form
\begin{equation}
    \theta_{MN} = X_N^+ Y_N.
\end{equation}
\end{frame}


\begin{frame}{Minimum norm solution norm behaviour}
The norm of the minimum norm using the test sample $||\theta_{N+1}|| = ||\hat{\theta}\left(z^N,x,y \right)||$
\pause
\begin{theorem}
Denote $||x_\bot||^2 = x^\top \left[I - X_N^\top (X_N X_N^\top )^{-1} X_N \right] x$,
the recursive equation of the norm of the minimum norm solution is given by 
\begin{equation}  \label{eq:mn_solution_norm}
||\theta_{N+1}||^2 = ||\theta_{MN}||^2 + \frac{1}{||x_\bot||^2}(y-x^\top \theta_{MN})^2.
\end{equation}
\end{theorem}

\note{
We have an initial results on the norm behaviour of the minimum norm solution. \\
x bot is the projection on the orthogonal space of the training set correlation matrix}
\end{frame}

\begin{frame}{Minimum norm solution norm behavior}
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/mn_norm_behaviour.pdf}
\end{center}
\vspace{-1cm}
\begin{itemize}
    \item For small $||x_\bot||^2$, a slight deviation from $x^\top \theta_{MN}$ increases significantly the norm
    \item For large $||x_\bot||^2$, deviating from $x^\top \theta_{MN}$ does not change the norm
\end{itemize}
\note{
If the test sample lies in the subspace of the training correlation matrix, $||x_\bot||^2$ becomes small and a slight deviation from the minimum norm prediction increases significantly the norm. \\
If the test sample lies in the orthogonal subspace, the denominator is relatively large and a deviation from the minimum norm does not change the norm. \\
Where there are lots of y with the same norm, all of them are reasonable and therefore we can't trust a specific one. \\
On the other hand, if there is just one model with a low norm, we are confident that it is the right one. \\
For confident prediction, we would like that any other prediction will cause a model with high complexity
}
\end{frame}

\begin{frame}{pNML with norm constraint}
\begin{itemize}
\item The hypothesis set:  $\Theta = \left\{\theta| \theta \in R^M, ||\theta|| = ||\theta_{MN}|| \right\}$
\item $\hat{\theta}(z^N,x,y,\lambda) = \left( X_{N+1}^\top  X_{N+1} + \lambda I\right)^{-1} X_{N+1}^\top Y_{N+1}$
\item For each possible test label we find its $\lambda$ that satisfies the equation
    \begin{equation} 
        ||\hat{\theta}(z^N,x,y,\lambda)||^2 = ||\theta_{MN}||^2
    \end{equation}
\item Compute the normalization factor
    \begin{equation}
    K
    = 
    \int_{-\infty}^\infty
    \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{-
    \frac{\left(y- x^\top \hat{\theta}(z^N,x,y,\lambda) \right)^2}{2 \sigma^2} \right\} dy.
    \end{equation}
\end{itemize}
\note{The hypothesis set contains learners that have a norm that is smaller or equals to the norm of the minimum norm solution. \\
When y is equal to the minimum norm we get $\lambda$ zero. \\
When y is far from the minimum norm prediction, $\lambda$ is greater.}
\end{frame}

\begin{frame}{pNML with norm constraint simulation}
\begin{itemize}
\item The training set consist of 4 points $\{(t_i,y_i)\}_{i=1}^4$ in the interval $[0, 1]$
\item Converting $t_i$ to features:
    \begin{equation}
    X_{N}[i,m]
    =
    \sqrt{\pi} \cos \left(\pi m t_i +  \frac{\pi}{2} m\right) .
    \end{equation}
\end{itemize}

\centering
\includegraphics[width=0.49\textwidth]{figures/overparam_pnml_linear_regression_figures/figure_pnml_linear_regression_mn_prediction.pdf}
\pause
\includegraphics[width=0.49\textwidth]{figures/overparam_pnml_linear_regression_figures/figure_pnml_linear_regression_mn_regret.pdf}

\note{
We simulate the constraint pNML to see if we get meaningful regret. \\
in the left the minimum norm prediction with the training set in red. \\
In the right the corresponds regret. \\
You can the that the regret is informative. \\
Around the training set the regret is low.}
\end{frame}

\subsection{Logistic regression}
\begin{frame}{Logistic regression}
\begin{itemize}
\item The model maps a test feature into binary class.
\item Denote $w$ as the learnable parameters, the probabilities of a test feature $x$ to have label '0' and '1' are:
\begin{equation}
    p_w(y=0|x,z^N) = \frac{e^{-w^\top x}}{1 + e^{-w^\top x}} = \frac{1}{1 + e^{w^\top x}}, \qquad
    p_w(y=1|x, z^N) = \frac{1}{1 + e^{-w^\top x}}.
\end{equation}
\item The ERM solution $w_N$ is usually obtained by minimizing the training log-loss using optimization techniques such as gradient descent
\begin{equation}
w_N = \argmin_{w} \left[ - \sum_{i=1}^N \log p(y_i|x_i) \right].
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{Logistic regression recursive solution}
\begin{itemize}
\item A recursive form of logistic regression using the linear regression minimum norm solution was recently suggested~\footfullcite{zhuang2020training}.
\item Denote $f(\cdot)$ as the activation function, 
\begin{equation}
    g = \left(x^\top \left(I - X^+_N X_N \right) \right)^+, \quad  z = f^{-1}(y),
\end{equation}
\item The recursive logistic regression formula is
\begin{equation} \label{eq:logistig_regression_recurisve}
    w_{N+1} = w_N + g \left(z - x^\top w_N \right).
\end{equation}
\end{itemize}
\end{frame}

\begin{frame}{Logistic regression pNML}
\begin{itemize}
\item The log normalization factor is the pNML regret:
\begin{equation}
    \log K = \log \left(p_{\hat{\theta}(z^N,x,y=0)}(y=0|x) + p_{\hat{\theta}(z^N,x,y=1)}(y=1|x)\right).
\end{equation}
\item Initial results show that the pNML regret using the recursive update rule is
\begin{equation}
    \Gamma = \log K = \log \left(1 + \frac{1}{1 +  \mathbf{1}_{||x_\bot|| = 0} \left[ 2\cosh{\left(x^\top w_N \right)} \right]}\right).
\end{equation}
\item If the test sample fully lies in the trainset subspace: $||x_\bot||=0$ and the regret is
\begin{equation}
\Gamma = \log \left(1 + \frac{1}{1 + 2\cosh{\left(x^\top w_N \right)}}\right).
\end{equation}
The regret is bounded by $0 \leq \Gamma \leq 1\frac{1}{3}$.
\item If there is a portion of the test sample that lies in the trainset orthogonal subspace, $||x_\bot||>0$ and the maximal possible regret is obtained.
\end{itemize}
\end{frame}

\begin{frame}{Logistic regression future research}
\begin{itemize}
\setlength\itemsep{2em}
\item Compare the regret of the recursive solution and the gradient descent based solution.
\item  Use this recursive method with the open-set classification task for accelerating the compute time of the pNML.
\item Develop the pNML regret for logistic regression with gradient decent.
\end{itemize}
\end{frame}

\subsection{Deep neural networks}
\begin{frame}{DNN future research}
\begin{itemize}
\setlength\itemsep{2em}
\item Using DNN the regret as a generalization measure might be useless.
\item Motivated by the pNML with norm constraint. We can apply a similar approach for DNN. 
\item Intrinsic dimension~\footfullcite{li2018measuring} is an example of quantifying the complexity of a DNN models.
\end{itemize}
\note{
The model has a perfect fit for each label. \\
We want to find a property that defines the complexity of the DNN model and use it to constrain the pNML learner. \\
Using these methods as a constraint in the pNML scheme might lead to a meaningful regret.
}
\end{frame}



\begin{frame}
\Huge{\centerline{The End}}
\end{frame}


% ------------------------------ %
% Appendix
% ------------------------------ %
\backupbegin
\begin{frame}[c]
\begin{center}
\Huge Appendix
\end{center}
\end{frame}

\begin{frame}{pNML Proof}
\begin{itemize}
\item Note that the regret is equal for all choices of $y$.
\item  If we consider a different probability assignment, it should assign a smaller probability for at least one of the possible outcomes. In this case, if the true label is one of those outcomes it will lead to a higher regret. 
\item The induced pNML min-max regret is:
\begin{equation} \label{individual_regret}
R^*(z^{N},x) = \log\sum_{y \in \mathcal{Y}}p_{\hat{\theta}(z^N,x,y)}(y|x), 
\end{equation}
\item This regret is greater than zero:
\begin{equation} \label{individual_regret_positive}
\begin{split}
\log\sum_{y \in \mathcal{Y}}p_{\hat{\theta}(z^N,x,y)}(y|x)
&\geq \log\sum_{y  \in \mathcal{Y}}p_{\hat{\theta}(z^N,x,y^*)}(y|x) = 0, \nonumber
\end{split}
\end{equation}
where we used the fact that the likelihood adjusted to each $y$ in the sum is greater than the likelihood associated with any fixed $y^*$, $p_{\hat{\theta}(z^N,x,y^*)}(y|x) \leq p_{\hat{\theta}(z^N,x,y)}(y|x)$, and that the sum over all possible $y$'s of probability distribution on $y$ with a fixed $\hat{\theta}(z^N,x,y^*)$ is $1$.
\end{itemize}
\end{frame}

\begin{frame}{Bayesian learning}
The relation between x and y is probabilistic and depends on an unknown model $\theta^*$:
\begin{equation}
    P{_\theta^*}(y|x).
\end{equation} 
Given hypothesis class $\Theta$, the goal is to find a learner $q$ that minimizes the regret
\begin{equation}
R(z^N,\theta^*,q) = \mathrm{E} \left[ \log \frac{p_{\theta^*}}{q} \right] = \sum_{Y_N,y \in \mathcal{Y}^{N+1}} p_{\theta^*}(Y_N,y|X_N,x) \log \frac{p_{\theta^*}(y|x)}{q (y|z^N,x)}.
\end{equation}
The min-max criteria
\begin{equation}
    \min_{\theta} \max_{\theta^*} R.
\end{equation}
The solution is a mixture
\begin{equation}
q(y|z^N,x) = \int_{
\theta} w(\theta|z^N) p_{\theta}(y|x) d \theta.
\end{equation}
\end{frame}


\begin{frame}{Minimum norm solution proof}
The objective
\begin{equation}
\textit{minimize} \quad \theta^\top \theta \quad \textit{subject to} \quad X \theta = Y
\end{equation}
The Lagrangian
\begin{equation}
L(\theta,\mu)= \theta^\top \theta + \mu^\top \left(X \theta - Y\right).
\end{equation}
The optimally conditions are
\begin{equation}
\frac{\partial L}{\partial \theta} = 2 \theta  + X^\top \mu = 0,
\qquad
\frac{\partial L}{\partial \mu} = X \theta - Y =0.
\end{equation}
From the first condition
\begin{equation}
\theta = - \frac{1}{2} X^\top \mu.
\end{equation}
Substitute it to the second condition,
\begin{equation}
\mu = - 2 \left(X X^\top \right)^{-1} Y
\end{equation}
And finaly
\begin{equation}
\theta = X^\top \left(X X^\top \right)^{-1} Y
\end{equation}
\end{frame}

\begin{frame}{Moore–Penrose inverse TODO}
\begin{equation}
\lim_{\lambda \xrightarrow{} 0 }\left(X^\top X  + \lambda I \right)X^\top = X X^\top\left(X X^\top\right)
\end{equation}
\end{frame}

\begin{frame}{Open-set competitors 1}
\begin{itemize}
\item 
Max-prob: \fullcite{hendrycks2016baseline}. \\
A naive method that assigns the confidence score based on the maximum probability of a trained DNN.
Input with a low maximum probability is considered to have an unknown class.
\item 
ODIN: ~\fullcite{liang2017enhancing}. \\
To increase the margin between the maximum softmax score of data with known classes and data with unknowns, this method pre-processes the input by perturbing it with the loss gradient.
The score is the DNN's output maximum probability when feeding it with the perturbed input.
\end{itemize}
\end{frame}

\begin{frame}{Open-set competitors 2}
\begin{itemize}
\item Entropic Open-Set: \fullcite{dhamija2018reducing}. \\
During training, a dataset with unknown classes $\Du'$ is used. 
The loss function is modified such that data from $\Du'$ produce a uniform probability vector and therefore will have a low maximum probability score.
The value of the maximum probability of the prediction is used to distinguish between images with known classes and inputs with unknowns.
Different unknowns sets ($\Du'$ and $\Du$) are used in the training and evaluation phases.
\item Objectosphere: \fullcite{dhamija2018reducing}. \\
Similarly to the Entropic Open-Set approach, a model is trained to have a low response to unknown classes.
An additional loss term is added that directly penalizes high norm features of OOD samples and encourages closed-set samples to have high features norm.
In the evaluation, the maximum probability value of the prediction is used as the confidence score.
\end{itemize}
\end{frame}

\begin{frame}{Open-set competitors 3}
\begin{itemize}
\item  
Cosine Similarity: \fullcite{wang2018cosface}. \\
This method proposes a change in the softmax layer.
The main idea is to use the cosine of the angle between the weights of the last fully connected layer that are associated with a specific class and the features as the logits that feed the softmax layer. 
Same as other methods, this approach uses the maximum probability for confidence.

\item 
Maximum Discrepancy~\fullcite{yu2019unsupervised}. \\
Two classifiers are used along with an additional OOD training set.
During training, a new loss term is adopted that increases the discrepancy between the classifiers on the OOD training set.
The confidence score is the $L_1$ distance between the classifiers' probability assignments.
\end{itemize}
\end{frame}

\begin{frame}{DNN complexity measure}
\fullcite{li2018measuring}:
Training networks with a small randomly oriented parameter subspace
\includegraphics[width=\textwidth]{figures/MEASURING THE INTRINSIC-DIMENSION-OF-OBJECTIVE- LANDSCAPES.png}
\end{frame}

\backupend
\end{document}

